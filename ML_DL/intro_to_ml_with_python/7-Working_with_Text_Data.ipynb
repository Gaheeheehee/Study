{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 텍스트 데이터 다루기\n",
    "# Working With Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 문자열 데이터 타입\n",
    "\n",
    "데이터 분석을 하면서 문자열 데이터를 접할 수 있는 경우는 아래와 같이 네 가지 종류가 있다.\n",
    "- 범주형 데이터\n",
    "- 범주에 의미를 연결시킬 수 있는 임의의 문자열\n",
    "- 구조화된 문자열 데이터\n",
    "- 텍스트 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__범주형 데이터__는 고정된 목록(개수)로 구성된다. 예를 들어 가장 좋아하는 색을 묻는 설문에서 `빨강, 녹색, 파랑`과 같이 하나를 선택하는 경우를 말한다.<br />\n",
    "__범주에 의미를 연결시킬 수 있는 임의의 문자열은__ \"사과 같이 빨간색\", \"불그스름한 색\" 은 \"빨강\"이라는 범주에 연결시킬 수 있는 문자열을 의미한다. <br />\n",
    "__구조화된 문자열 데이터__는 범주형 데이터 같은 데이터는 아니지만, 주소나 장소, 이메일, 전화번호 등 처럼 일정한 구조를 가진 데이터를 말한다. <br />\n",
    "__텍스트 데이터__는 뉴스의 기사, 댓글, 논문 등과 같이 일반적인 텍스트 데이터를 말한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 분석에서 데이터 셋을 ***말뭉치(corpus)***라 하고, 하나의 텍스트를 의미하는 각 데이터 포인트를 ***문서(document)***라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 예제 애플리케이션: 영화 리뷰 감성 분석 (Sentiment Analysis)\n",
    "이 교재에서는 텍스트 분석 예제로 스탠포드 대학교 연구원인 앤드류 마스가 IMDB(Internet Movie DataBase)에서 수집한 영화 리뷰 데이터셋을 사용한다. 해당 데이터 셋은 http://ai.stanford.edu/~amaas/data/sentiment/ 에서 다운 받을 수 있다. 다운로드 후 압축을 풀면 약 400MB 크기의 데이터가 있고 아래와 같이 구성되어 있다. IMDb 사이트에는 1 ~ 10까지 점수가 있다. 이 데이터셋은 7점이상은 positive, 4점 이하는 negative인 이진 분류 데이터셋으로 구분 되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새 볼륨 볼륨에 대한 폴더 경로의 목록입니다.\n",
      "볼륨 일련 번호가 00000028 CC7F:4282입니다.\n",
      "D:\\USERS\\CJH\\DEV\\STUDY\\STUDY\\ML_DL\\INTRO_TO_ML_WITH_PYTHON\\DATA\\ACLIMDB\n",
      "├─test\n",
      "│  ├─neg\n",
      "│  └─pos\n",
      "└─train\n",
      "    ├─neg\n",
      "    └─pos\n"
     ]
    }
   ],
   "source": [
    "## Windows10 - 64bit\n",
    "!tree data/aclImdb\n",
    "\n",
    "## Mac OS\n",
    "# !tree -dL 2 data/aclImdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pos`폴더에는 긍정적인(positive) 리뷰가 각각 하나의 파일로 나위어 있고, `neg` 폴더(negative)에도 마찬가지로 있다. `unsup` 폴더는 레이블이 없는 데이터를 담고 있는데, 실습에서는 필요하지 않으므로 삭제해준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `data/aclImdb/train/unsup': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Mac OS\n",
    "!rm -r data/aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn`의 `load_files`함수를 사용해서 파일을 읽어 올 수 있다. `load_files`로 폴더의 데이터를 읽을 때 레이블은 폴더의 알파벳 순서에 따라 0부터 부여한다. 여기서의 데이터셋은 neg 폴더에는 레이블이 0이되고, pos는 1이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "\n",
    "%time reviews_train = load_files('./data/aclImdb/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_train의 타입 : <class 'list'>\n",
      "text_train의 길이 : 25000\n",
      "text_train[6]: \n",
      "b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "print('text_train의 타입 : {}'.format(type(text_train)))\n",
    "print('text_train의 길이 : {}'.format(len(text_train)))\n",
    "print('text_train[6]: \\n{}'.format(text_train[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html 태그 제거 - <br />태그\n",
    "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 샘플 수 (Training data): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "print('클래스별 샘플 수 (Training data): {}'.format(np.bincount(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 같은 방법으로 Test data를 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 문서 수: 25000\n",
      "클래스별 샘플 수 (테스트 데이터): [12500 12500]\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reviews_test = load_files(\"./data/aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"테스트 데이터의 문서 수: {}\".format(len(text_test)))\n",
    "print(\"클래스별 샘플 수 (테스트 데이터): {}\".format(np.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 텍스트 데이터를 BOW로 표현하기\n",
    "머신러닝에서 텍스트를 표현하는 방법 중 ***BOW(Bag Of Words)*** 는 간단하면서도 효과적이어서 많이 사용한다. 하지만, BOW를 사용할 경우, 텍스트 데이터는 텍스트 자체의 문장, 문단과 같은 텍스트 구조는 사라지게 되며 단어의 빈도수만 확인 가능하다. BOW를 표현하기 위해서는 아래의 세 가지 단계가 필요하다. <br />\n",
    "1. **토큰화(tokenization)**: 각 문서(문장)을 단어(토큰)으로 나누는 작업을 말한다.\n",
    "2. **어휘 사전 구축**: 모든 문서에 나타난 모든 단어의 어휘를 모으고 번호를 매긴다(알파벳 순서)\n",
    "3. **인코딩**: 어휘 사전의 단어가 문서마다 몇 번이나 나타나는지를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 샘플 데이터에 BOW 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 리뷰 데이터가 아닌 임의의 데이터\n",
    "\n",
    "bards_words = [\"The fool doth think he is wise,\",\n",
    "              \"but the wise man knows himself to be a fool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(bards_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 사전의 크기: 13\n",
      "어휘 사전의 내용:\n",
      " {'man': 8, 'he': 4, 'himself': 5, 'wise': 12, 'doth': 2, 'fool': 3, 'be': 0, 'but': 1, 'is': 6, 'knows': 7, 'think': 10, 'the': 9, 'to': 11}\n"
     ]
    }
   ],
   "source": [
    "print(\"어휘 사전의 크기: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"어휘 사전의 내용:\\n {}\".format(vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플데이터에 대해 BOW형태로 만들려면 `transform`메소드를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words)\n",
    "print(\"BOW: {}\".format(repr(bag_of_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW의 밀집 표현:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BOW의 밀집 표현:\\n{}\".format(bag_of_words.toarray()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 영화 리뷰에 대한 BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CounterVectorizer`객체의 `get_feature_name`메소드는 각 특성에 해당하는 단어를 리스트로 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "특성 개수: 74849\n",
      "처음 20개 특성:\n",
      "['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830', '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s', '01', '01pm', '02']\n",
      "20,010에서 20,030까지 특성:\n",
      "['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback', 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl', 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\n",
      "매 2,000번쨰 특성:\n",
      "['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful', 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher', 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse', 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "print('특성 개수: {}'.format(len(feature_names)))\n",
    "print('처음 20개 특성:\\n{}'.format(feature_names[:20]))\n",
    "print(\"20,010에서 20,030까지 특성:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"매 2,000번쨰 특성:\\n{}\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특성 추출을 위한 전처리를 적용하기 전에 현재의 BOW를 이용하여 분류기를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.7 s\n",
      "CV Avg. Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%time scores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\n",
    "print(\"CV Avg. Score: {:.2f}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression`에 있는 파라미터 `C`를 조정하기 위해 그리드 서치를 이용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 50s\n",
      "최상의 CV 점수: 0.89\n",
      "최적의 파라미터:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "%time grid.fit(X_train, y_train)\n",
    "print(\"최상의 CV 점수: {:.2f}\".format(grid.best_score_))\n",
    "print(\"최적의 파라미터: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 점수: 0.88\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"테스트 점수: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 사용한 `CounterVectorizer`는 정규표현식을 사용해 토큰을 추출한다. 기본적으로 사용하는 정규표현식은 \"\\b\\w\\w+\\b\"이다. \\b로 경계를 구분하고 \\w로 적어도 둘 이상의 문자나 숫자가 연속된 단어를 찾아 토큰을 추출한다. <br />\n",
    "`min_df` 파라미터로 토큰이 나타날 최소 문서 개수를 지정해 줄 수 있다 아래의 코드는 최소 문서 개수를 `min_df=5`로 설정해 주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_df로 제한한 X_train: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"min_df로 제한한 X_train: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features:\n",
      "['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107', '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120', '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16', '160', '1600', '16mm', '16s', '16th']\n",
      "Features 20010 to 20030:\n",
      "['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions', 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement', 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying', 'replays', 'replete', 'replica']\n",
      "Every 700th feature:\n",
      "['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered', 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds', 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate', 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking', 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea', 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "print(\"First 50 features:\\n{}\".format(feature_names[:50]))\n",
    "print(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\n",
    "print(\"Every 700th feature:\\n{}\".format(feature_names[::700]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 볼 수 있듯이 최소 문서 개수를 설정하는 것만으로도 희귀한 단어와 철자가 틀린 단어들이 사라진것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 교차 검증 점수: 0.89\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 교차 검증 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 불용어(Stop-Words)\n",
    "불용어는 영어에서 `and, is, there,...`나 한글에서 `은, 는, 이, 가,...`등과 같이 텍스트 분석에 있어 의미를 가지지 않는 유용하지 않은 단어들을 제외하는 것을 말한다. scikit-learn에서는 `feature_extraction.text` 모듈에 영어의 불용어를 가지고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수: 318\n",
      "매 10번째 불용어:\n",
      "['down', 'anyhow', 'anyway', 'beside', 'although', 'six', 'but', 'anyone', 'whereupon', 'rather', 'herein', 'couldnt', 'sixty', 'part', 'who', 'latter', 'mostly', 'few', 'us', 'see', 'co', 'former', 'i', 'such', 'empty', 'be', 'amongst', 'almost', 'thin', 'may', 'either', 'get']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(\"불용어 개수: {}\".format(len(ENGLISH_STOP_WORDS)))\n",
    "print(\"매 10번째 불용어:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어가 제거된 X_train:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "print(\"불용어가 제거된 X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 크로스 밸리데이션 점수: 0.88\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `max_df`를 사용해서 자주 나타나는 단어를 제거한 뒤 테스트를 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import make_pipeline\n",
    "# pipe = make_pipeline(CountVectorizer(), LogisticRegression())\n",
    "# param_grid = {'countvectorizer__max_df': [100, 1000, 10000, 20000], 'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "# grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "# grid.fit(text_train, y_train)\n",
    "# print(\"최상의 크로스 밸리데이션 점수: {:.2f}\".format(grid.best_score_))\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 TF-IDF 로 데이터 스케일 변경하기\n",
    "TF-IDF는 정보 검색(Information Retrieval)과 텍스트 마이닝에서 사용하는 가중치이다.\n",
    "여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치를 말한다.\n",
    "- **TF(Term Frequency)**: 단어빈도로 특정 단어가 문서 내에 얼만큼의 빈도로 등장하는지를 나타내는 척도\n",
    "- **IDF(Inverse Document Frequency)**: 역문헌 빈도수로 문서 빈도의 역수. 전체 문서 개수를 해당 단어가 포함된 문서의 개수로 나눈 것을 의미 \n",
    "\n",
    "Scikit-learㅇn에서는 `TfidfTransformer`와 `TfidfVectorizer` 이렇게 두 개의 클래스에 TF-IDF가 구현되어 있다.\n",
    "- `TfidfTransformer`는 `ConterVectorizer`가 만든 희소행렬을 입력받아 변환한다.\n",
    "- `TfidfVectorizer`는 텍스트 데이터를 입력받아 BOW와 tf-idf 변환을 수행한다.\n",
    "- `TfidfTransformer`와 `TfidfVectorizer` 둘 다 계산식은 아래와 같다.\n",
    "$$tfidf(w,d)=tf(\\log(\\frac{N+1}{N_w + 1})+1)$$\n",
    "\n",
    "- `TfidfTransformer`와 `TfidfVectorizer` 두 클래스 모두 tf-idf 계산 후 L2 정규화(L2 normalization)을 적용한다. 따라서 유클리드 놈(euclidean norm)이 1이 되도록 각 문서 벡터의 스케일을 바꿔준다. 이렇게 되면 문서의 길이(단어의 수)에 영향을 받지 않게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최상의 CV 점수: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(text_train, y_train)\n",
    "print('최상의 CV 점수: {:.2f}'.format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 낮은 tfidf를 가진 특성:\n",
      "['suplexes' 'gauche' 'hypocrites' 'oncoming' 'songwriting' 'galadriel'\n",
      " 'emerald' 'mclaughlin' 'sylvain' 'oversee' 'cataclysmic' 'pressuring'\n",
      " 'uphold' 'thieving' 'inconsiderate' 'ware' 'denim' 'reverting' 'booed'\n",
      " 'spacious']\n",
      "가장 높은 tfidf를 가진 특성: \n",
      "['gadget' 'sucks' 'zatoichi' 'demons' 'lennon' 'bye' 'dev' 'weller'\n",
      " 'sasquatch' 'botched' 'xica' 'darkman' 'woo' 'casper' 'doodlebops'\n",
      " 'smallville' 'wei' 'scanners' 'steve' 'pokemon']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "# 훈련 데이터 셋을 변환한다.\n",
    "X_train = vectorizer.transform(text_train)\n",
    "# 특성별로 가장 큰 값을 찾는다.\n",
    "max_value = X_train.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "# 특성 이름을 구한다.\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(\"가장 낮은 tfidf를 가진 특성:\\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[:20]]))\n",
    "\n",
    "print(\"가장 높은 tfidf를 가진 특성: \\n{}\".format(\n",
    "      feature_names[sorted_by_tfidf[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 낮은 idf를 가진 특성:\n",
      "['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n",
      " 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n",
      " 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n",
      " 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n",
      " 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n",
      " 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n",
      " 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n",
      " 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n",
      " 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n",
      " 'him']\n"
     ]
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(vectorizer.idf_)\n",
    "print(\"가장 낮은 idf를 가진 특성:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
